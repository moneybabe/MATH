\documentclass{book}
\input{preamble.tex}
\input{macros.tex}

\title{\Huge{MATH 110 Notes}\\\emph{Book: Linear Algebra Done Right (4th Edition)}}
\author{\huge{Neo Lee}}
\date{\huge{Fall 2023}}

\begin{document}

\maketitle
\let\cleardoublepage\clearpage
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents

%---------------NEW CHAPTER---------------
\chapter{Vector Spaces}
\section{$R^n$ and $C^n$}
\section{Definition of Vector Space}
\section{Subspaces}



%---------------NEW CHAPTER---------------
\chapter{Finite-Dimensional Vector Spaces}
\section{Span and Linear Independence}
\section{Bases}
\section{Dimension}





%---------------NEW CHAPTER---------------
\chapter{Linear Maps}
\section{Vector Space of Linear Maps}
\section{Null Spaces and Ranges}
\section{Matrices}
\section{Invertibility and Isomorphisms}
\section{Products and Quotients of Vector Spaces (Skipped)}
\section{Duality}





%---------------NEW CHAPTER---------------
\chapter{Polynomials}
%---------------NEW SECTION---------------
\section{A quick introduction to polynomials}
\dfn{Complex numbers}{
    Suppose $z=a+bi$, where $a$ and $b$ are real numbers.
    \begin{itemize}
        \item The \emph{real part} of $z$, is defined by Re\,$z=a$.
        \item The \emph{imaginary part} of $z$, is defined by Im\,$z=b$.
    \end{itemize}
}
\dfn{Complex conjugate and absolute value}{
    Suppose $z\in\C$.
    \begin{itemize}
        \item The \emph{complex conjugate} of $z$, denoted by $\overline{z}$, is defined by 
        \[
            \overline{z}=\t{Re}\,z-(\t{Im}\,z)i.
        \]
        \item The \emph{absolute value} of a complex number $z$, denoted by $\abs{z}$, is defined by 
        \[
            \abs{z}=\sqrt{(\t{Re\,}z)^2 + (\t{Im\,}z)^2} = \sqrt{z\overline{z}}.
        \]
    \end{itemize}
}
\mprop{Properties of complex numbers}{
    Suppose $w,z\in C$. Then the following equalites and inequalities hold.
    \begin{itemize}
        \item 
        \tb{sum of $z$ and $\overline{z}$}
        \[
            z+\overline{z}=2\t{Re\,}z.   
        \]
        \item
        \tb{difference of $z$ and $\overline{z}$}
        \[
            z-\overline{z}=2(\t{Im\,}z)i.
        \]
        \item
        \tb{product of $z$ and $\overline{z}$}
        \[
            z\overline{z}=\abs{z}^2.
        \]
        \item 
        \tb{additivity and multiplicativity of complex conjugate}
        \[
            \overline{z+w}=\overline{z}+\overline{w},\qquad \overline{zw}=\overline{z}\,\overline{w}.
        \]
        \item
        \tb{double conjugate}
        \[
            \overline{\overline{z}}=z.
        \]
        \item
        \tb{real and imaginary parts are bounded by $\abs{z}$}
        \[
            \abs{\t{Re\,}z}\leq\abs{z},\qquad \abs{\t{Im\,}z}\leq\abs{z}.
        \]
        \item
        \tb{absolute value of the complex conjugate}
        \[
            \abs{\overline{z}}=\abs{z}.
        \]
        \item
        \tb{multiplicativity of absolute value}
        \[
            \abs{wz}=\abs{w}\abs{z}.
        \]
        \item 
        \tb{triangle inequality}
        \[
            \abs{w+z}\leq\abs{w}+\abs{z}.
        \]
    \end{itemize}
}

\subsection{Division Algorithm for Polynomials}
\mprop{If a polynomial is the zero function, then all coefficients are 0}{
    Suppose $a_0,\dots, a_m\in \F$. If 
    \[
        a_0 + a_1z + \dots + a_mz^m = 0  
    \]
    for every $z\in F$, then $a_0=\cdots=a_m=0$.
    \nt{
        \nt{
            Think of polynomial as a function that maps from a domain to a codomain, and the 
            actions of this function can be encapsulated by a list of coefficients.
        }
        This result implies that the coefficients of a polynomial are uniquely determined because 
        if a polynomial had two different sets of coefficients, then subtracting the two 
        representations of the polynomial would give a contradiction to the result.
    }
}
\mlemma{Division algorithm for polynomials}{
    Suppose that $p,s\in\p(\F)$, with $s\neq0$. Then there exist unique polynomials 
    $q,r\in\p(\F)$ such that 
    \[
        p=sq+r  
    \]
    and $\deg r<$ $\deg s$.
}

\subsection{Zeros of Polynomials}
\dfn{Zero of a polynomial}{
    A number $\lambda\in F$ is called a \emph{zero} (or root) of a polynomial $p\in p(\F)$ if 
    \[
        p(\lambda)=0.    
    \]  
}
\dfn{Factor}{
    A polynomial $s\in \p(\F)$ is called a \emph{factor} of $p\in \p(\F)$ if there exists a 
    polynomial $q\in\p(\F)$ such that $p=sq$.
}
\mprop{Each zero of a polynomial corresponds to a degree-one factor}{
    Suppose $p\in\p(\F)$ and $\lambda\in\F$. Then $p(\lambda)=0$ if and only iff there is a polynomial 
    $q\in\p(\F)$ such that 
    \[
        p(z)  =(z-\lambda)q(z)
    \]
    for every $z\in\F$.
    \nt{
        $(z-\lambda)$ is called a \emph{linear factor} of $p$. It is also a degree one polynomial.
    }
}
\mprop{A polynomial has at most as many zeros as its degree}{
    Suppose $p\in\p(F)$ is a polynomial with degree $m\geq0$. Then $p$ has at most $m$ 
    distinct zeros in $\F$.
}

\subsection{Factorization of Polynomials over \C}
\thm{Fundamental theorem of algebra, first version}{
    Every nonconstant polynomial with complex coefficients has a zero in $\C$.
}
\thm{Fundamental theorem of algebra, second version}{
    If $p\in\p(\C)$ is a nonconstant polynomial, then $p$ has a unique factorization 
    (up to rearrangement of the factors) of the form
    \[
        p(z)=c(z-\lambda_1)\cdots(z-\lambda_m)
    \]
    where $c,\lambda_1,\dots,\lambda_m\in\C$ and $c\neq0$.
}

\subsection{Factorization of Polynomials over $\R$}
\nt{
    We like to use $x$ to denote the real variable of a polynomial over \R, and $z$ to denote the 
    complex variable of a polynomial over \C.
}
\mprop{Polynomials with real coefficients have nonreal zeros in pairs}{
    Suppose $p\in\p(\C)$ is a polynomial with real coefficients. If $\lambda\in\C$ is a zero of 
    $p$, then so is $\overline{\lambda}$.
}
\mprop{Factorization of a quadratic polynomial}{
    Suppose $b,c\in\R$. Then there is a polynomial factorization of the form 
    \[
        x^2+bx+c=(x-\lambda_1)(x-\lambda_2)  
    \]
    with $\lambda_1,\lambda_2\in\R$ if and only if $b^2-4c\geq0$.
}
\thm{Factorization of a polynomial over \R}{
    Suppose $p\in\p(\R)$ is a nonconstant polynomial. Then $p$ has a unique factorization 
    (up to rearrangement of the factors) of the form
    \[
        p(x)=c(x-\lambda_1)\cdots(x-\lambda_m)(x^2+b_1x+c_1)\cdots(x^2+b_nx+c_n)
    \]
    where $c,\lambda_1,\dots,\lambda_m,b_1,\dots,b_n,c_1,\dots,c_n\in\R$, $c\neq0$, and
    $b_k^2-4c<0$ for each $k$.
}





%---------------NEW CHAPTER---------------
\chapter{Eigenvalues and Eigenvectors}
%---------------NEW SECTION---------------
\section{Invairant Subspaces}
\subsection{Eigenvalues}
\dfn{Operator}{
    A linear map from a vector space to itself is called an \emph{operator}.
    \nt{
        We denote $\L(V)$ as the set of all operators on $V$, which is the same as $\L(V,V)$.
    }
}
\dfn{Invariant subspace}{
    Suppose $T\in\L(V)$. A subspace $U$ of $V$ is called \emph{invariant} under $T$ if
    \[
        Tu\in U \qquad \t{for every }u\in U.
    \]
}
\ex{Invariant subspace of differentiation operator}{
    Suppose $V=\p(\F)$ and $D\in\L(V)$ is the differentiation operator. Then $U=\p_n(\F)$ is
    an invariant subspace of $D$ for every $n\in\N$ because for every $v\in\p(\F)$, 
    $Dv$ is a polynomial of degree one less than $v$, and hence $Dv\in\p_n(\F)$.
}
\ex{Four invariant subspaces, not necessarily all different}{
    If $T\in\L(V)$, then the following subspaces are all invariant under $T$:
    \begin{itemize}
        \item $\{0\}$.
        \item $V$.
        \item $\nul T$.
        \item $\range T$.
    \end{itemize}
}
\dfn{Eigenvalue}{
    Suppose $T\in\L(V)$. A number $\lambda \in\F$ is called an \emph{eigenvalue} of $T$ if there
    exists a nonzero vector $v\in V$ such that $Tv=\lambda v$.
}
\ex{Eiganvalue in $\R^3$}{
    Define an opeartor $T\in\L(\R^3)$ by 
    \[
        T(x,y,z)=(7x+3z, 3x+6y+9z, -6y).
    \]
    Then 
    \[
        T(3,1,-1)  =(18,6,-6)=6(3,1,-1).
    \]
    Thus 6 is an eigenvalue of $T$.
}
\mprop{Equivalent conditions to be an eigenvalue}{
    Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $\lambda\in\F$. Then the following are
    equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $\lambda$ is an eigenvalue of $T$.
        \item $T-\lambda I$ is not injective.
        \item $T-\lambda I$ is not surjective.
        \item $T-\lambda I$ is not invertible.
    \end{enumerate}
    \pf{Proof}{
        $Tv=\lambda v$ is equivalent to $(T-\lambda I)v=0$, which means $T-\lambda I$ (which is 
        an operator on $V$) has non-trivial null space, and all the equivalent conditions follow 
        from the rank-nullity theorem.
    }
}
\dfn{Eigenvector}{
    Suppose $T\in\L(V)$ and $\lambda\in\F$ is an eigenvalue of $T$. A vector $v\in V$ is called 
    an \emph{eigenvector} of $T$ corresponding to $\lambda$ if $v\neq0$ and $Tv=\lambda v$.
    \nt{
        Because $Tv = \lambda v$ if an only if $(T-\lambda I)v=0$, a vector $v\in V$ with 
        $v\neq0$ is an eigenvector of $T$ corresponding to $\lambda$ if and only if 
        $v\in\nul(T-\lambda I)$.
    }
}
\mlemma{Linearly independent eigenvectors}{
    Suppose $T\in\L(V)$. Then every list of eigenvectors of $T$ corresponding to \emph{distinct}
    eigenvalues is linearly independent.
}
\mlemma{Operator cannot have more eigenvalues than dimension of vector space}{
    Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct
    eigenvalues.
}

\subsection{Polynomials Applied to Operators}
\dfn{Notation: $T^m$}{
    Suppose $T\in\L(V)$ and $m$ is a positive integer.
    \begin{itemize}
        \item $T^m\in\L(V)$ is defined by $T^m=\underbrace{T\cdots T}_{m\t{ times}}$.
        \item $T^0$ is defined to be the identity operator $I$ on $V$.
        \item If $T$ is invertible with inverse $T^{-1}$, then $T^{-m}\in\L(V)$ is defined by 
        $T^{-m}=(T^{-1})^m$.
    \end{itemize}
}
\dfn{Notation: $\p(T)$}{
    Suppose $T\in\L(V)$ and $p\in\p(\F)$ is a polynomial given by 
    \[
        p(z)=a_0+a_1z+\dots+a_mz^m \qquad \t{for all }z\in\F.
    \]
    Then $\p(T)\in\L(V)$ is defined by
    \[
        \p(T)=a_0I+a_1T+\dots+a_mT^m.
    \]
    \nt{
        If we fix an opeartor $T\in\L(V)$, then the fucntion $\Phi:\p(\F)\to\L(V)$ given 
        by $p\mapsto p(T)$ is linear because 
        \[
            \Phi(p+q)=(p+q)(T)=p(T)+q(T)=\Phi(p)+\Phi(q),\qquad \Phi(cp)=cp(T)=c\Phi(p)
        \]
        for every $p,q\in\p(\F)$ and $c\in\F$, which can be checked by writing out the coefficients
        of $p(T)$ and $q(T)$.
    }
}
\ex{A polynomial applied to the differentiation opeartor}{
    Suppose $D\in\L(\p(\R))$ is the differentiation operator and $p\in\p(\R)$ is given by
    \[
        p(x)=7-3x+5x^2.
    \]
    Then 
    \[
        \l(p(D)\r)q=7q-3Dq+5D^2q=7-3q'+5q'' \qquad \t{for all }q\in\p(\R).
    \]
}
\dfn{Product of polynomials}{
    If $p,q\in\p(\F)$, then $pq\in\p(\F)$ is the polynomial defined by 
    \[
        (pq)(z)=p(z)q(z) \qquad \t{for all }z\in\F.
    \]
}
\mprop{Multiplicative properties}{
    Suppose $p,q\in\p(\F)$ and $T\in\L(V)$. Then 
    \begin{enumerate}[label=(\alph*)]
        \item $(pq)(T) = p(T)q(T).$
        \item $p(T)q(T) = q(T)p(T).$
    \end{enumerate}
}
\mprop{Null space and range of $p(T)$ are invariant under $T$}{
    Suppose $T\in\L(V)$ and $p\in\p(\F)$. Then $\nul p(T)$ and $\range p(T)$ are invariant under $T$.
}



%---------------NEW SECTION---------------
\newpage
\section{Eigenvalues and the Minimal Polynomial}
\subsection{Existence of Eigenvalues on Complex Vector Space}
\thm{Operators on complex vector spaces have an eigenvalue}{
    Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
}

\subsection{Minimal Polynomial}
\dfn{Monic polynomial}{
    A \emph{monic polynomial} is a polynomial whose highest-degree coefficient equals 1.
}
\mprop{Existence, uniqueness, and degree of minimal polynomial}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Then there is a unique monic polynomial 
    $p\in\p(\F)$ of smallest degree such that $p(T)=0$. Futhermore, $\deg p\leq\dim V$.
    \nt{
        Because the existence and uniquess of such monic polynomial is guaranteed, we can define
        a \emph{minimal polynomial} of $T$.
    }
}
\dfn{Minimal polynomial}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Then the \emph{minimal polynomial} of $T$ 
    is the unique monic polynomial $p\in\p(\F)$ of smallest degree such that $p(T)=0$.
    \nt{
        To compute the minimal polynomial of an operator $T\in\L(V)$, we need to find the smallest 
        positive integer $m$ such that the equation
        \[
            c_0I+c_1T+\dots+c_{m-1}T^{m-1}=-T^m
        \]
        has a solution $(c_0,\dots,c_{m-1})\in\F$. We can pick a basis of $V$ and represent $T$ as
        a matrix, then the equation above can be thought of as a system of $(\dim V)^2$ linear equations, and
        we can use Gaussian elimination to see if a unique solution exists for successive values of 
        $m=1,\dots,\dim V-1$ until a unique solution is found.

        Even faster (usually), pick $v\in V$ (can be a basis vector) and consider the equation 
        \[
            c_0v+c_1Tv+\dots+c_{\dim V-1}T^{\dim V-1}v=-T^{\dim V}v.
        \]
        Use a basis of $V$ to represent the vectors $T^kv$ in the equation above, then it is 
        a system of $\dim V$ linear equations. If this system has a unique 
        solution $c_0,c_1,\dots,c_{\dim V-1}$ (as happens most of the time), then this unique 
        solution is the coefficients of the minimal polynomial of $T$ because the uniqueness of the 
        solution guarantees that the degree of the minimal polynomial is $\dim V$ and the
        minimal polynomial can have degree at most $\dim V$.
    }
}
\thm{Eigenvalues are the zeros of the minimal polynomial}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$.
    \begin{enumerate}[label=(\alph*)]
        \item The zeros of the minimal polynomial of $T$ are the eigenvalues of $T$.
        \item If $V$ is a complex vector space, then the minimal polynomial of $T$ has the form 
        \[
            p(z)=(z-\lambda_1)^{m_1}\cdots(z-\lambda_k)^{m_k}  
        \]
        where $\lambda_1,\dots,\lambda_k$ are the distinct eigenvalues of $T$ and $m_1,\dots,m_k$
        are positive integers.
    \end{enumerate}
    \nt{
        \begin{itemize}
            \item
            The result in (b) is only true for complex vector spaces because only $\C$ can guarantee 
            a linear factorization of a polynomial.
            \item 
            The powers in (b) mean that the minimal polynomial of $T$ may or may not have 
            repeated factors.
            \item
            (a) and (b) combined to gether means that all the zeros of the minimal polynomial of $T$
            are eigenvalues of $T$, and all the eigenvalues of $T$ are zeros of the minimal polynomial
            of $T$.
        \end{itemize}

    }
}
\mprop{$q(T)=0\iff q$ is a polynomial multiple of the minimal polynomial}{
    Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $q\in\p(F)$. Then $q(T)=0$ if and only 
    if $q$ is a polynomial multiple of the minimal polynomial of $T$.
    \nt{
        This means that the characteristic polynomial of $T$ is a polynomial multiple of the
        minimal polynomial of $T$.
    }
}
\mprop{Minimal polynomial of a restriction operator}{
    Suppose $V$ is finite-dimensional, $T\in\L(V)$, and $U$ is a subspace of $V$ that is invariant 
    under $T$. Then the minimal polynoial of $T$ is a polynomimal multiple of the minimal polynomial 
    of $T|_U$.
    \pf{Proof}{
        The minimal polynomial of $T$ acting on $T|_U$ is also a zero function, and the previous 
        proposition implies that the minimal polynomial of $T$ is a polynomial multiple of the
        minimal polynomial of $T|_U$.
    }
}
\mprop{$T$ not invertible $\iff$ constant term of minimal polynomial of $T$ is 0}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Then $T$ is not invertible if and only if
    the constant term of the minimal polynomial of $T$ is 0.
    \pf{Proof}{
        Denote the minimal polynomial of $T$ by $p$. 
        \begin{align*}
            T \t{ is not invertible} &\iff T \t{ is not injective} \\
            &\iff \nul T\neq \{0\} \\
            &\iff 0 \t{ is an eigenvalue of } T \t{ with eigenspace} = \nul T \\
            &\iff 0 \t{ is a zero of } p \\
            &\iff p(0) = 0 \\
            &\iff \t{the constant term of } p \t{ is } 0.
        \end{align*}
    }
}

\subsection{Real Vector Spaces: Invariant Subspaces and Eigenvalues}
\thm{Eigenvalue or invariant subspace of dimension 2}{
    Every operator on a finite-dimensional non zero vector space has an invariant subspace with 
    dimension 1 or dimension 2.
}
\thm{Operators on odd-dimensional vector spaces have eigenvalues}{
    Every operator on an odd-dimensional vector space has an eigenvalue.
}



%---------------NEW SECTION---------------
\newpage
\section{Upper-Triangular Matrices}
\subsection{Definitions}
\dfn{Matrix of an operator, $\M(T)$}{
    Suppose $T\in\L(V)$ and $v_1,\dots v_n$ is a basis of $V$. The matrix of $T$ with respect to
    this basis is the $n$-by-$n$ matrix 
    \[
        \M(T) = \begin{bmatrix}
            A_{1,1} & \cdots & A_{1,n} \\
            \vdots & \ddots & \vdots \\
            A_{n,1} & \cdots & A_{n,n}
        \end{bmatrix}  
    \]
    whose entries $A_{j,k}$ are defined by 
    \[
        Tv_k = A_{1,k}v_1+\dots+A_{n,k}v_n.
    \]
    If the basis is not clear from the context, then the notation $\M(T, (v_1,\dots,v_n))$ is used.
}
\dfn{Diagonal of a matrix}{
    The \emph{diagonal} of a square matrix consists of the entries on the line from the upper left 
    corner to the bottom right corner.
    \nt{
        Only square matrices have diagonals.
    }
}
\dfn{Upper-triangular matrix}{
    A square matrix is called \emph{upper triangular} if all entries below the diagonal are zero.
}

\subsection{Conditions for Upper-Triangular Matrices}
\mprop{Conditions for upper-triangular matrix}{
    Suppose $T\in\L(V)$ and \vbasisn is a basis of $V$. Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item $\M(T, (\vbasisn))$ is upper triangular.
        \item $\spann(\vbasisk)$ is invariant under $T$ for each $k=1,\dots,n$.
        \item $Tv_k\in\spann(\vbasisk)$ for each $k=1,\dots,n$.
    \end{enumerate}
}
\thm{Determination of eigenvalues from upper-triangular matrix}{
    Suppose $T\in\L(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then the
    eigenvalues of $T$ are the entries on the diagonal of this matrix.
    \nt{
        If $\M(T,\vbasisn)$ is an upper-triangular matrix, $v_1$ is angenvector of $T$. However, 
        $v_2,\dots,v_n$ need not be eigenvectors of $T$. A basis vector $v_k$ is an eigenvector 
        of $T$ if and only if all the entries in the $k^{\t{th}}$ column of the matrix are 0, except 
        possibly the $k^{\t{th}}$ entry.
    }
}
\thm{Necessary and sufficient condition to have an upper-triangular matrix}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Then $T$ has an upper-triangular matrix with
    respect to some basis of $V$ if and only if the minimal polynomial of 
    $T$ equals $(z-\lambda_1)\cdots(z-\lambda_n)$ for some $\lambda_1,\dots,\lambda_n\in\F$. 
    \nt{
        In other words, $T$ has an upper-triangular matrix if and only if the minimal polynomial of $T$
        can be linearly factorized. (The linear factors can be repeated.)
    }
}
\thm{If $\F=\C$, then every operator on $V$ has an upper-triangular matrix}{
    Suppoes $V$ is finite-dimensional and $T\in\L(V)$.
    \begin{enumerate}[label=(\alph*)]
        \item If $\F=\C$, then $T$ has an upper-triangular matrix with respect to some basis of $V$.
        \item If $\F=\R$, then $T$ has an upper-triangular matrix with respect to some basis of $V$
        if and only if every zero of the minimal polynomial of $T$, thought of as a polynomial with 
        complex coefficients, is real.
    \end{enumerate}
    \pf{Proof}{
        Follows directly from the previous proposition and the fundamental theorem of algebra and 
        its consequence on factorization of polynomial over $\C$ and $\R$.
    }
}
\nt{
    If starting with a square matrix, the matrix in row echelon form will be an upper-triangular 
    matrix. However, do not confuse this upper-triangular matrix with the upper-triangular matrix 
    of an operator with respect to some basis of $V$. There is no connection between the two.
}



%---------------NEW SECTION---------------
\newpage
\section{Diagonalizable Operators}
\subsection{Definitions}
\dfn{Diagonal matrix}{
    A \emph{diagonal matrix} is a square matrix that is zero everywhere except possibly on the
    diagonal.
}
\dfn{Diagonalizable}{
    An operator $T\in\L(V)$ is called \emph{diagonalizable} if the operator has a diagonal matrix
    with respect to some basis of $V$.
    \nt{
        This is true if and only if $V$ has a basis of eigenvectors of $T$.
    }
}
\dfn{Eigenspace, $E(\lambda, T)$}{
    Supose $T\in\L(V)$ and $\lambda\in\F$. The \emph{eigenspace} of $T$ corresponding to $\lambda$
    is the subspace $E(\lambda, T)$ of $V$ defined by
    \[
        E(\lambda, T) = \nul(T-\lambda I) = \{v\in V:Tv=\lambda v\}.
    \]
    In other words, $E(\lambda, T)$ is the set of all eigenvectors of $T$ corresponding to $\lambda$,
    along with the zero vector.
    \nt{
        If $\lambda$ is an eigenvalue of $T$, then $T$ restricted to $E(\lambda, T)$ is just 
        multiplication by $\lambda$.
    }
}

\subsection{Conditions for Diagonalizability}
\mprop{Sum of eigenspaces is a direct sum}{
    Suppose $T\in\L(V)$ and $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$. Then 
    \[
        E(\lambda_1, T)+\dots+E(\lambda_m, T)  
    \]
    is a direct sum. further more, if $V$ is finite-dimensional, then
    \[
        \dim E(\lambda_1, T)+\dots+\dim E(\lambda_m, T) \leq \dim V.
    \]
}
\mprop{Conditions equivalent to diagonalizability}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Let $\lambda_1,\dots,\lambda_m$ denote the 
    distinct eigenvalues of $T$. Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item $T$ is diagonalizable.
        \item $V$ has a basis consisting of eigenvectors of $T$.
        \item $V = E(\lambda_1, T)\oplus\dots\oplus E(\lambda_m, T)$.
        \item $\dim V = \dim E(\lambda_1, T)+\dots+\dim E(\lambda_m, T)$.
    \end{enumerate}
}
\mprop{Enough eigenvalues implies diagonalizability}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$ has $\dim V$ distinct eigenvalues. Then 
    $T$ is diagonalizable.
}
\thm{Necessary and sufficient condition for diagonalizability}{
    Suppose $V$ is finite-dimensional and $t\in\L(V)$. Then $T$ is diagonalizable if and only if 
    the minimal polynomial of $T$ equals $(z-\lambda_1)\cdots(z-\lambda_m)$ for some 
    list of distinct numbers $\lambda_1,\dots,\lambda_m\in\F$.
    \nt{
        In other words, $T$ is diagonalizable if and only if the minimal polynomial of $T$ can be 
        linearly factorized into distinct linear factors.
    }
}
\mprop{Restriction of diagonalizable to invariant subspace is diagonalizable}{
    Suppose $T\in\L(V)$ is diagonalizable and $U$ is a subspace of $V$ that is invariant under 
    $T$. Then $T|_U$ is a diagonalizable operator on $U$.
    \pf{Proof}{
        Denote the minimal polynomial of $T|_U$ as $q$ and the minimal polynomial of $T$ as $p$. 
        Then $p$ is a polynomial multiple of $q$ because $p(T|_U)=0$. Since $p$ can be linearly
        factorized, $q$ can also be linearly factorized, otherwise $p$ as a polynomial multiple of
        $q$ would have repeated factors. Thus $T|_U$ is diagonalizable.
    }
}



%---------------NEW SECTION---------------
\newpage
\section{Commuting Operators}
\dfn{Commute}{
    \begin{itemize}
        \item Two operators $S,T\in\L(V)$ are said to \emph{commute} if $ST=TS$.
        \item Two square matrices $A,B\in\F^{n\times n}$ are said to \emph{commute} if $AB=BA$.
    \end{itemize}
    \nt{
        Commuting matrices are unusual. For example, there are 214,358,881 pairs of $2\times2$
        matrices all of whose entries are integers in the interval $[-5,5]$. About 0.3\% of these
        pairs of matrices commute.
    }
}
\mprop{Commuting operators correspond to commuting matrices}{
    Suppose $S,T\in\L(V)$ and \vbasisn is a basis of $V$. Then $S$ and $T$ commute if and only if
    $\M(S,(\vbasisn))$ and $\M(T,(\vbasisn))$ commute.
}
\mlemma{Eigenspace is invariant under commuting operator}{
    Suppose $S,T\in\L(V)$ commute and $\lambda\in\F$. Then $E(\lambda, S)$ is invariant under $T$.
}
\thm{Simultaneous diagonalizability $\iff$ commutativity}{
    Two diagonalizable operators on the same vector space have diagonal matrices with respect to 
    the same basis if and only if they commute.
}
\mprop{Common eigenvector for commuting operators}{
    Every pair of commuting operatos on a finite-dimensional, nonzero, complex vector space has 
    a common eigenvector.
}
\thm{Commuting operators are simultaneously upper triangularizable}{
    Suppose $V$ is a finite-dimensional complex vector space and $S,T$ are commuting operators on 
    $V$. Then there is a basis of $V$ with respect to which both $S$ and $T$ have upper-triangular 
    matrices.
}
\thm{Eigenvalues of sum and product of commuting operators}{
    Suppose $V$ is a finite-dimensional complex vector space and $S,T$ are commuting operators on 
    $V$. Then 
    \begin{itemize}
        \item every eigenvalue of $S+T$ is an eigenvalue of $S$ plus an eigenvalue of $T$.
        \item every eigenvalue of $ST$ is a an eigenvalue of $S$ times an eigenvalue of $T$.
    \end{itemize}
}





%---------------NEW CHAPTER---------------
\chapter{Inner Product Spaces}
%---------------NEW SECTION---------------
\section{Inner Products and Norms}
\subsection{Inner Products}
\dfn{Inner product}{
    An \emph{inner product} on $V$ is a function that takes each ordered pair $(u,v)$ of elements 
    of $V$ to a number $\inner{u,v}\in\F$ and has the following properties.
    \begin{itemize}
        \item \tb{positivity}
        \[
            \inner{v,v}\geq0 \qquad \t{for all }v\in V.
        \]
        \item \tb{definiteness}
        \[
            \inner{v,v}=0 \iff v=0.
        \]
        \item \tb{additivity in the first slot}
        \[
            \inner{u+v,w}=\inner{u,w}+\inner{v,w} \qquad \t{for all }u,v,w\in V.
        \]
        \item \tb{homogeneity in the first slot}
        \[
            \inner{cu,v}=c\inner{u,v} \qquad \t{for all }u,v\in V \t{ and }c\in\F.
        \]
        \item \tb{conjugate symmetry}
        \[
            \inner{u,v}=\overline{\inner{v,u}} \qquad \t{for all }u,v\in V.
        \]
    \end{itemize}
}
\dfn{Inner product space}{
    An \emph{inner product space} is a vector space equipped with an inner product on $V$.
}
\mprop{Basic properties of an inner product}{
    \begin{enumerate}[label=(\alph*)]
        \item For each fixed $u\in V$, the function that maps $v\in V$ to $\inner{u,v}$ is a linear
        map from $V$ to $F$, specifically a linear functional on $V$.
        \item $\inner{0, v} = \inner{v, 0}$ for all $v\in V$.
        \item $\inner{u, v+w}=\inner{u,v}+\inner{u,w}$ for all $u,v,w\in V$.
        \item $\inner{u, \lambda v}=\overline{\lambda}\inner{u,v}$ for all $u,v\in V$ and $\lambda\in\F$.
    \end{enumerate}
}

\subsection{Norms}
\dfn{Norm, $\norm{v}$}{
    For $v\in V$, the \emph{norm} of $v$, denoted $\norm{v}$, is defined by 
    \[
        \norm{v} = \sqrt{\inner{v,v}}.
    \]
}
\mprop{Basic properties of the norm}{
    Suppose $v\in V$.
    \begin{enumerate}[label=(\alph*)]
        \item $\norm{v}=0\iff v=0$.
        \item $\norm{\lambda v} = |\lambda|\norm{v}$ for all $\lambda\in\F$.
    \end{enumerate}
}
\dfn{Orthogonal}{
    Two vectors $u,v\in V$ are called \emph{orthogonal} if $\inner{u,v}=0$.
    \nt{
        Hence, orthogonality is dependent on the inner product.
    }
}
\mprop{Orthogonality and 0}{
    \begin{enumerate}[label=(\alph*)]
        \item 0 is orthogonal to every vector in $V$.
        \item 0 is the only vector in $V$ that is orthogonal to itself.
    \end{enumerate}
}

\subsection{Consequent Properties of Inner Products and Norms}
\thm{Pythagorean theorem}{
    Suppose $u,v\in V$. If $u$ and $v$ are orthogonal, then
    \[
        \norm{u+v}^2 = \norm{u}^2+\norm{v}^2.
    \]
    \nt{
        This is equivalent to saying their self inner products (norm squared) can be split into the 
        sum of the self inner products of each vector.
        \[
            \inner{u+v,u+v} = \inner{u,u}+\inner{v,v}.
        \]
        But this is not necessarily true for the norm of the sum of two vectors.
    }
}
\mprop{An orthogonal decomposition}{
    Suppose $u,v\in V$, with $v\neq 0$. Set $c=\frac{\inner{u,v}}{\norm{v}^2}$ and 
    $w=u-cv$. Then 
    \[
        \inner{w,v} = 0 \t{ and } u=w+cv.  
    \]
}
\thm{Cauchy-Schwarz inequality}{
    Suppose $u,v\in V$. Then
    \[
        |\inner{u,v}|\leq\norm{u}\norm{v}.  
    \]
    This inequality is an equality if and only if $u$ and $v$ are linearly dependent.
}
\thm{Triangle inequality}{
    Suppose $u,v\in V$. Then
    \[
        \norm{u+v}\leq\norm{u}+\norm{v}.  
    \]
    This inequality is an equality if and only if on of $u$ and $v$ is a nonnegative multiple of
    the other.
}
\thm{Reverse triangle inequality}{
    Suppose $u,v\in V$. Then
    \[
        \big|\norm{u}-\norm{v}\big|\leq\norm{u-v}.  
    \]
    This inequality is an equality if and only if one of $u$ and $v$ is a nonnegative multiple of
    the other.
}
\thm{Parallelogram equality}{
    Suppose $u,v\in V$. Then
    \[
        \norm{u+v}^2+\norm{u-v}^2=2\norm{u}^2+2\norm{v}^2.
    \]
    \pf{Proof}{
        Just expand by the definition of norm.
    }
}



%---------------NEW SECTION---------------
\newpage
\section{Orthonormal Bases}
\subsection{Orthonormal Lists and the Gram-Schmidt Procedure}
\dfn{Orthonormal}{
    \begin{itemize}
        \item A list of vectors is called \emph{orthonormal} if each vector in the list has 
        norm 1 and is orthogonal to all the other vectors in the list.
        \item In other words, a list $e_1,\dots,e_m$ is orthonormal if and only if 
        \[
            \inner{e_i,e_j} = \begin{cases}
                1 & \t{if }i=j \\
                0 & \t{if }i\neq j.
            \end{cases}  
        \]
    \end{itemize}
}
\mprop{Norm of an orthonormal linear combination}{
    Suppose \ebasism is an orthonormal list of vectors in $V$. Then 
    \[
        \norm{a_1e_1+\cdots+a_me_m}^2 = |a_1|^2+\cdots+|a_m|^2 \qquad \t{for all }a_1,\dots,a_m\in\F.  
    \]
    \pf{Proof}{
        It follows directly from a repeated application of the Pythagorean theorem and the basic 
        properties of the norm.
    }
}
\mcor{Orthonormal lists are linearly independent}{
    Every orthonormal list of vectors is linearly independent.
    \nt{
        This implies every orthogonal list of vectors (excluding the 0 vector) is linearly 
        independent.
    }
}
\thm{Bessel's inequality}{
    Suppose $\ebasism$ is an orthonormal list of vectors in $V$. If $v\in V$ then
    \[
        |\inner{v,e_1}|^2 +\cdots + |\inner{v,e_m}|^2 \leq \norm{v}^2.  
    \]
}
\dfn{Orthonormal basis}{
    An \emph{orthonormal basis} of $V$ is a basis of $V$ that is orthonormal.
}
\mprop{Orthonormal lists of the right length are orthonormal bases}{
    Suppose $V$ is finite-dimensional. Then every orthonormal list of vectors in $V$ with 
    $\dim V$ is an orthonoral basis of $V$.
}
\mprop{Writing a vector as linear combination of orthonromal basis}{
    Suppose $\ebasisn$ is an orthonormal basis of $V$ and $u,v\in V$. Then
    \begin{enumerate}[label=(\alph*)]
        \item $v = \inner{v,e_1}e_1 + \cdots + \inner{v,e_n}e_n$.
        \item $\norm{v}^2 = |\inner{v,e_1}|^2 + \cdots + |\inner{v,e_n}|^2$.
        \item $\inner{u,v} = \inner{u,e_1}\overline{\inner{v,e_1}} + \cdots + 
        \inner{u,e_n}\overline{\inner{v,e_n}}$.
    \end{enumerate}
    \pf{Proof}{
        The first equality is from the orthogonal decomposition. The second equality is from 
        (a) and Pythagorean theorem. The third equality is by taking the 
        inner product of $u$ with each side of (a) and then using the conjugate symmetry.
    }
}
\thm{Gram-Schmidt procedure}{
    Suppose $\vbasism$ is a linearly independent list of vectors in $V$. 
    Let $e_1 = \frac{v_1}{\norm{v_1}}$. For $k=2,\dots,m$, define 
    $e_k$ inductively by 
    \[
        e_k = \frac{v_k-\inner{v_k,e_1}e_1-\cdots-\inner{v_k,e_{k-1}}e_{k-1}}
        {\norm{v_k-\inner{v_k,e_1}e_1-\cdots-\inner{v_k,e_{k-1}}e_{k-1}}}.
    \]
    Then $\ebasism$ is an orthonormal list of vectors in $V$ such that 
    \[
        \spann(\vbasisk) = \spann(\ebasisk) \qquad \t{for } k=1,\dots,m.
    \]
    \nt{
        We can split this procedure into two steps. First, we find residual of $v_k$ after projecting 
        $v_k$ onto the subspace spanned by $e_1,\dots,e_{k-1}$
        \[
            u_k = v_k - \inner{v_k,e_1}e_1-\cdots\inner{v_k,e_{k-1}}e_{k-1},
        \]
        then we normalize $u_k$ to get $e_k$ by
        \[
            e_k = \frac{u_k}{\norm{u_k}}.  
        \]
    }
}
\mprop{Existence of orthonormal basis}{
    Every finite-dimensional inner product space has an orthonormal basis.
    \pf{Proof}{
        Choose a basis of $V$ and apply Gram-Schmidt procedure, which will obtain an orthonormal
        list with the same span as the original basis.
    }
}
\mprop{Every orthonormal list extends to orthonoral basis}{
    Suppose $V$ is finite-dimensional. Then every orthonormal list of vectors in $V$ can be extended 
    to an orthonormal basis of $V$.
    \pf{Proof}{
        Suppose $\ebasism$ is an orthonormal list of vectors in $V$, then we extend this orthonormal 
        hence linearly independent list of vectors to a basis $\ebasism, \vbasisn$ of $V$. Now we apply 
        Gram-Schmidt procedure to $\ebasism, \vbasisn$, which the first $\ebasism$ will remain 
        unchanged since they are already orthonormal. Then we will obtain 
        \[
            \ebasism, \ubasisn,  
        \]
        where the list is orthonormal and spans $V$.
    }
}
\mprop{Upper-triangular matrix with respect to orthonormal basis}{
    Suppose $V$ is finite-dimensional and $T\in\L(V)$. Then $T$ has an upper-triangular matrix 
    with respect to some orthonormal basis of $V$ if and only if the minimal polynomial of $T$
    equals $(z-\lambda_1)\cdots(z-\lambda_n)$ for some $\lambda_1,\dots,\lambda_n\in\F$.
    \pf{Proof}{
        We show $T$ is upper-triangularizable with respect to some basis if and only if 
        $T$ is upper-triangularizable with respect to some orthonormal basis. Then the result
        follows from the necessary and sufficient conditions of upper-triangularizable matrix.

        Suppose $T$ is upper-triangularizable with respect to some basis of $V$. Then 
        $\spann(\vbasisk)$ is invariant under $T$ for each $k=1,\dots,n$ by the equivalent conditions 
        of upper-triangular matrix. Then we apply Gram-Schmidt procedure to $\vbasisk$ to obtain
        an orthonormal basis $\ebasisk$ of $V$. These two list of vectors have the same span 
        \[
            \spann(\vbasisk) = \spann(\ebasisk) \qquad \t{for } k=1,\dots,n.  
        \]
        Hence, $\spann(\ebasisk)$ is invariant under $T$ for each $k=1,\dots,n$. By the 
        equivalent conditions of upper-triangular matrix again, $T$ is upper-triangularizable
        with respect to the orthonormal basis $\ebasisn$.

        The other direction is always true.
    }
}
\thm{Schur's theorem}{
    Every operator on a finite-dimensional complex inner product space has an upper-triangular
    matrix with respect to some orthonormal basis.
    \pf{Proof}{
        This is a direct consequence of the previous proposition and fundamental theorem of 
        algebra that all polynomials can be linearly factorized over $\C$.
    }
}

\subsection{Linear Functionals on Inner Product Spaces}
\dfn{Linear functional, dual space, $V'$}{
    \begin{itemize}
        \item A \emph{linear functional} on $V$ is a linear map from $V$ to $\F$.
        \item The \emph{dual space} of $V$, denoted $V'$, is the vector space of all linear Functionals
        on $V$, in other words, $V'=\L(V,\F)$.
    \end{itemize}
}
\thm{Riesz representation theorem}{
    Suppose $V$ is finite-dimensional and $\varphi$ is a linear functional on $V$. Then 
    there is a unique vector $v\in V$ such that 
    \[
        \varphi(u) = \inner{u,v} \qquad \t{for all }u\in V.
    \]
    \pf{Proof}{
        Let $\ebasisn$ be an orthonormal basis of $V$, then 
        \begin{align*}
            \varphi(u) & = \varphi(\inner{u,e_1}e_1+\cdots+\inner{u,e_n}e_n) \\
            & = \inner{u,e_1}\varphi(e_1)+\cdots+\inner{u,e_n}\varphi(e_n) \\
            & = \inner{u,\overline{\varphi(e_1)}e_1+\cdots+\overline{\varphi(e_n)}e_n}.
        \end{align*}
        Hence, the vector $v$ is 
        \[
            v = \overline{\varphi(e_1)}e_1+\cdots+\overline{\varphi(e_n)}e_n.  
        \]

        Now we prove the uniqueness of $v$. Suppose there is another vector $w\in V$ such that
        \[
            \varphi(u) = \inner{u,w} \qquad \t{for all }u\in V.
        \]
        Then 
        \[
            \inner{u, v} - \inner{u, w} = \inner{u, v-w} = 0 \qquad \t{for all }u\in V.
        \]
        Take $u = v-w$, then the above equation is equivalent to 
        \[
            \norm{v-w}^2 = 0,
        \]
        which by the definiteness of the inner product implies $v-w=0\iff v=w$.
    }
    \nt{
        The uniqueness of the Riesz representation theorem is very useful in proving the 
        adjoint of an operator in later chapters.
    }
}



%---------------NEW SECTION---------------
\newpage
\section{Orthogonal Complements and Minimization Problems}
\subsection{Orthogonal Complements}
\dfn{Orthogonal complement, $U^\perp$}{
    If $U$ is a subset of $V$, then the \emph{orthogonal complement} of $U$, denoted $U^\perp$, is 
    the set of all vectors in $V$ that are orthogonal to every vector in $U$:
    \[
        U^\perp = \{v\in V:\inner{u,v}=0 \t{ for all }u\in U\}.  
    \]
}
\mprop{Properties of orthogonal complement}{
    \begin{enumerate}[label=(\alph*)]
        \item If $U$ is a subset of $V$, then $U^\perp$ is a subspace of $V$.
        \item $\{0\}^\perp = V$.
        \item $V^\perp = \{0\}$.
        \item If $U$ is a subset of $V$, then $U\cap U^\perp\subseteq\{0\}$.
        \nt{
            The intersection can be empty if $U$ does not contain the zero vector.
        }
        \item If $G$ and $H$ are subsets of $V$ and $G\subseteq H$, then $H^\perp\subseteq G^\perp$.
    \end{enumerate}
}
\mprop{Direct sum of a subspace and its orthogonal complement}{
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \[
        V = U\oplus U^\perp.  
    \]
}
\mprop{Dimension of orthogonal complement}{
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \[
        \dim U + \dim U^\perp = \dim V.  
    \]
}
\mprop{Orthogonal complement of the orthogonal complement}{
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \[
        (U^\perp)^\perp = U.  
    \]
}
\mprop{$U^\perp=\{0\}\iff U=W$ (for $U$ a finite-dimensional subspace of $W$)}{
    Suppose $U$ is a finite-dimensional subspace of $V$. Then 
    \[
        U^\perp=\{0\} \iff U=W.
    \]
}
\dfn{Orthogonal projection, $P_U$}{
    Suppose $U$ is a finite-dimensional subspace of $V$. The \emph{orthogonal projection} of $V$ 
    onto $U$ is the operator $P_U\in\L(V)$ defined as follows: For each $v\in V$, 
    write $v=u+w$, where $u\in U$ and $w\in U^\perp$. Then $P_Uv=u$.
    \nt{
        The direct sum decomposition $V=U\oplus U^\perp$ guarantees thet every vector $v\in V$ can
        be written uniquely as $v=u+w$, where $u\in U$ and $w\in U^\perp$.
    }
}
\mprop{Properties of orthogonal projection $P_U$}{
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \begin{enumerate}[label=(\alph*)]
        \item $P_U\in\L(V)$.
        \item $P_Uu=u$ for all $u\in U$.
        \item $P_Uw=0$ for all $w\in U^\perp$.
        \item $\range P_U$ = $U$.
        \item $\nul P_U = U^\perp$.
        \item $v-P_Uv\in U^\perp$ for all $v\in V$.
        \item $P_U^2=P_U$.
        \item $\norm{P_Uv}\leq\norm{v}$ for all $v\in V$.
        \item If $\ebasism$ is an orthonormal basis of $U$ and $v\in V$, then
        \[
            P_Uv = \inner{v,e_1}e_1+\cdots+\inner{v,e_m}e_m.  
        \]
    \end{enumerate}
}
\thm{Riesz representation theorem, revisited}{
    Suppose $V$ is finite-dimensional. For each $v\in V$, define $\varphi_v\in V'$ by
    \[
        \varphi_v(u) = \inner{u,v} \qquad \t{for all }u\in V.
    \]
    Then the map $V\to V'$ defined by $v\mapsto\varphi_v$ is injective.
    \nt{
        This map is not isophormism because this is not a linear map if $\F=\C$ due to 
        \[
            \lambda v\mapsto \varphi_{\lambda v} = \inner{u,\lambda v} = 
            \overline{\lambda}\inner{u,v} = \overline{\lambda}\varphi_v \neq \lambda\varphi_v.
        \]
    }
}

\subsection{Minimization Problems}
\mprop{Minimizing distance to a subspace}{
    Suppose $U$ is a finite-dimensional subspace of $V$, $v\in V$, and $u\in U$. Then
    \[
        \norm{v-P_Uv}\leq\norm{v-u}.
    \]
    Furthermore, the inequality above is an equality if and only if $u=P_Uv$.
    \nt{
        This is equivalent to saying the distance from $v$ to $U$ is minimized when $u=P_Uv$. Or 
        the residual of $v$ after orthogonal projection is the smallest among all residuals from 
        other kinds of projection.
    }
    \pf{Proof}{
        \begin{align*}
            \norm{v-P_Uv}^2 & \leq \norm{v-P_Uv}^2 + \norm{P_Uv-u}^2 \\
            & = \norm{(v-P_Uv) + (P_Uv-u)}^2 \\
            & = \norm{v-u}^2.
        \end{align*}
        The second equality holds because $v-P_Uv\in U^\perp$ and $P_Uv-u\in U$, hence they are 
        orthogonal to each other and we can apply the Pythagorean theorem.
    }
}

\subsection{Pseudoinverses (skipped)}
\mprop{Restrictoin of a linear map to obtain a bijective function}{
    Suppose $V$ is finite-dimensional and $T\in\L(V,W)$. Then $T|_{(\nul T)^\perp}$ is a bijective map 
    of $(\nul T)^\perp$ onto $\range T$.
}
\dfn{Pseudoinverse, $T^\dagger$}{
    Suppose that $V$ is finite dimensional and $T\in\L(V,W)$. The \emph{pseudoinverse} 
    $T^\dagger\in\L(W,V)$ of $T$ is the linear map from $W$ to $V$ defined by 
    \[
        T^\dagger w = (T|_{(\nul T)^\perp})^{-1}P_{\range T}w \qquad \t{for all }w\in W.
    \]
    \nt{
        \begin{itemize}
            \item The pseudoinverse is also called the \emph{Moore-Penrose inverse}.
            \item The pseudoinverse is equivalent to first projecting $w\in W$ to the $\range T$, 
            denote the projection as $u$,
            then applying the inverse of $T|_{(\nul T)^\perp}$ on $u$ to get the 
            pre-image of $u$ in the subspace $(\nul T)^\perp$. The inverse of 
            $T|_{(\nul T)^\perp}$ is guaranteed by the previous proposition.
        \end{itemize}
    }
}
\mprop{Algebraic properties of the pseudoinverse}{
    Suppose $V$ is finite-dimensional and $T\in\L(V,W)$.
    \begin{enumerate}[label=(\alph*)]
        \item If $T$ is invertible, then $T^\dagger=T^{-1}$.
        \item $T^\dagger T = P_{(nul T)^\perp} =$ the orthogonal projection of $V$ onto $(\nul T)^\perp$.
        \item $TT^\dagger = P_\range T =$ the orthogonal projection of $W$ onto $\range T$.
    \end{enumerate}
    \nt{
        The second and third property can be understood intuitively by drawing a diagram 
        involving the subspaces.
    }
}
\thm{Pseudoinverse provides best approximate solution or best solution}{
    Suppose $V$ is finite-dimensional, $T\in\L(V,W)$, and $b\in W$.
    \begin{enumerate}[label=(\alph*)]
        \item If $x\in V$, then
        \[
            \norm{T(T^\dagger b)-b}\leq\norm{Tx-b},
        \]
        with equality if and only if $x\in T^\dagger b + \nul T$.
        \item If $x\in T^\dagger b+\nul T$, then
        \[
            \norm{T^\dagger b} \leq \norm{x},
        \]
        with equality if and only if $x=T^\dagger b$.
    \end{enumerate}
    \nt{
        \begin{itemize}
            \item The first property means that $T^\dagger$ applied to $b$ gives a $x\in V$ such that 
            $Tx\in \range T$ is closest to $b$ among all $w\in \range T$. 
            \item The second property means that out of all $x\in V$ such that $Tx$ is closest to $b$, 
            $T^\dagger b$ has the smallest norm. 
        \end{itemize}
    }
}


%---------------NEW CHAPTER---------------
\chapter{Operators on Inner Product Spaces}
%---------------NEW SECTION---------------
\section{Self-Adjoint and Normal Operators}
\subsection{Adjoints}
\dfn{Adjoint, $T^*$}{
    Suppose $T\in\L(V,W)$. The \emph{adjoint} of $T$ is the function $T^*:W\to V$ such that 
    \[
        \inner{Tv,w} = \inner{v,T^*w} \qquad \t{for all }v\in V \t{ and }w\in W.
    \]
}
\mprop{Adjoint of a linear map is a linear map}{
    If $T\in\L(V,W)$, then $T^*\in\L(W,V)$.
}
\mprop{Properties of the adjoint}{
    Suppose $T\in\L(V,W)$. Then
    \begin{enumerate}[label=(\alph*)]
        \item $(S+T)^*=S^*+T^*$ for all $S\in\L(V,W)$.
        \item $(\lambda T)^*=\overline{\lambda}T^*$ for all $\lambda\in\F$.
        \item $(T^*)^*=T$.
        \item $(ST)^*=T^*S^*$ for all $S\in\L(W,U)$ (here $U$ is an arbitrary inner product space).
        \item $I^*=I$.
        \item if $T$ is invertible, then $T^*$ is invertible and $(T^*)^{-1}=(T^{-1})^*$.
    \end{enumerate}
}
\mprop{Null space and range of $T^*$}{
    Suppose $T\in\L(V,W)$. Then 
    \begin{enumerate}[label=(\alph*)]
        \item $\nul T^*=(\range T)^\perp$;
        \item $\range T^*=(\nul T)^\perp$;
        \item $\nul T=(\range T^*)^\perp$;
        \item $\range T=(\nul T^*)^\perp$.
    \end{enumerate}
}
\dfn{Conjugate transpose, $A^*$}{
    The \emph{conjugate transpose} of an $m$-by-$n$ matrix $A$ is the $n$-by-$m$ matrix $A^*$
    obtained by interchanging the rows and columns and then taking the complex conjugate of each 
    entry, In other words, $(A^*)_{j,k}=\overline{A_{k,j}}$.
}
\mprop{Matrix of $T^*$ equals conjugate transpose of matrix of $T$}{
    Let $T\in\L(V,W)$. Suppose $\ebasisn$ is an orthonormal basis of $V$ and $f_1,\dots,f_m$ is an 
    orthonormal basis of $W$. Then $\M(T^*,(f_1,\dots,f_m),(\ebasisn))$ is the conjugate tranpose 
    of $\M(T,(\ebasisn),(f_1,\dots,f_m))$. In other words,
    \[
        \M(T^*) = (\M(T))^*. 
    \]
}





\section{Spectral Theorem}
\section{Positiver Operators}
\section{Isometries, Unitary Operators, QR Factorization}
\section{Singular Value Decomposition}
\section{Consequences of Singular Value Decomposition}





%---------------NEW CHAPTER---------------
\chapter{Operators on Complex Vector Spaces}
\section{Generalized Eigenvectors and Nilpotent Operators}
\section{Generalized Eigenspace Decomposition}
\section{Consequences of Generalized Eigenspace Decomposition}
\section{Trace: A Connection Between Operators and Matrices}





%---------------NEW CHAPTER---------------
\chapter{Multilinear Algebra and Determinants (Skipped)}


\end{document}
