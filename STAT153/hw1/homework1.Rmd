---
title: 'Homework 1: Neo Lee'
subtitle: 'Introduction to Time Series, Fall 2023'
date: 'Due Tuesday September 5 at 5pm'
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

```{r, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, cache.comments = TRUE)
```

The total number of points possible for this homework is 41. The number of 
points for each question is written below, and questions marked as "bonus" are 
optional. Submit the **knitted html file** from this Rmd to Gradescope. 

If you collaborated with anybody for this homework, put their names here:

# Correlation and independence

1. (3 pts)
Give an example to show that two random variables can be uncorrelated but not 
independent. You must explicitly prove that they are uncorrelated but not
independent (for the latter, you may invoke any property that you know is
equivalent to independence).

SOLUTION GOES HERE

Consider a uniform random variable $X$ over the space of $\{-1, 0, 1\}$, and another random variable 
$Y = |X|$. Then, $Cov(X, Y) = E[XY] - E[X]E[Y] = (1 \times P(X = 1, Y = 1) - 1 \times 
P(X = -1, Y = 1) + 0\times P(X = 0, Y = 0)) - 0\times \frac{2}{3} = 0$. Hence, their 
correlation is also 0. However, we know $X, Y$ are definitely not independent, because we have 
explicitly defined $Y = |X|$. We can also check by looking at $P(Y = 1 | X = -1) = 1 \neq P(Y = 1) 
= \frac{2}{3}$.

2. (2 pts)
If $(X,Y)$ has a multivariate Gaussian distribution, and $X,Y$ are uncorrelated: 
$\mathrm{Cov}(X,Y) = 0$, then show that $X,Y$ are independent.


SOLUTION GOES HERE

We just have to show that $f_{X, Y}(x, y) = f_X(x)\cdot f_Y(y)$. Now we evaluate the joint 
distribution
\begin{align*}
    f_{X, Y}(x, y) & = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}\cdot exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_X)^2}{\sigma^2_X}-2\rho\frac{(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}+\frac{(y-\mu_Y)^2}{\sigma_Y^2}\right]\right) \\
    & = \frac{1}{2\pi\sigma_X\sigma_Y}\cdot exp\left(-\frac{1}{2}\left[\frac{(x-\mu_X)^2}{\sigma^2_X}+\frac{(y-\mu_Y)^2}{\sigma_Y^2}\right]\right) \qquad (\rho = 0 \because X, Y \textit{ are uncorrelated}) \\
    & = \frac{1}{\sqrt{2\pi}\sigma_X}\cdot exp\left(-\frac{1}{2}\left[\frac{(x-\mu_X)^2}{\sigma^2_X}\right]\right) \cdot \frac{1}{\sqrt{2\pi}\sigma_Y}\cdot exp\left(-\frac{1}{2}\left[\frac{(y-\mu_Y)^2}{\sigma_Y^2}\right]\right) \\
    & = f_X(x)\cdot f_Y(y).
\end{align*}

3. (3 pts)
Give an example to show that two random variables $X,Y$ can be marginally 
Gaussian (meaning, $X$ is Gaussian, and $Y$ is Gaussian) and uncorrelated but
*not* independent. Hint: $(X,Y)$ cannot be multivariate Gaussian in this case.

SOLUTION GOES HERE

Define $X \sim N(0, 1)$. Then define 
\begin{align*}
    Y = \begin{cases}
        X & \textit{with probability } \frac{1}{2} \\
        -X & \textit{with probability } \frac{1}{2}.
    \end{cases}
\end{align*}
We can verify that $Y$ is Gaussian by checking its *CDF*
\begin{align*}
    P(Y \le y) & = P(X \le y | Y = X)P(Y = X) + P(-X \le y | Y = -X)P(Y = -X) \\
    & = P(X \le y)\cdot\frac{1}{2} + P(X \ge -y)\cdot\frac{1}{2} \qquad (\textit{notice $P(X \le y) = P(X \ge -y)$}) \\
    & = P(X \le y).
\end{align*}
Hence, $Y \sim N(0, 1)$. Now we check that $X, Y$ are uncorrelated by evaluating their covariance
\begin{align*}
    Cov(X, Y) & = E[XY] - E[X]E[Y] \\
    & = E[XY] - 0 \\
    & = E[XY | Y = X]P(Y = X) + E[XY | Y = -X]P(Y = -X) \\
    & = E[X^2]\cdot\frac{1}{2} + E[-X^2]\cdot\frac{1}{2} \\
    & = 0.
\end{align*}
However, we know $X, Y$ are not independent, because $P(Y \le -1 | -0.1 \le X \le 0.1) = 0 \neq 
P(Y \le -1)$.

# Random walks

4. (4 pts)
Let $x_t$, $t = 1,2,3,\dots$ be a random walk with drift: 
\[
x_t = \delta + x_{t-1} + \epsilon_t,
\]
where (say) $\epsilon_t \sim N(0,\sigma^2)$ for $t = 1,2,3,\dots$. Suppose that
both $\delta$ and $\sigma^2$ are unknown. Devise a test statistic for the null
hypothesis that $\delta = 0$. This should be based on a standard test that you 
know (have learned in a past course) for testing whether the mean of Gaussian is
zero, with unknown variance, based on i.i.d.\ samples from this Gaussian.  

    State what the null distribution is for this test statistic, and how you 
would compute it in R (a function name is sufficient if the test statistic is 
implemented as a function in base R). Hint: consider taking differences along 
the sequence ... after that, what you want sounds like "c-test", or "p-test",
or "$\phi$-test", or ...

SOLUTION GOES HERE

We will proceed by testing the difference of the random walk sequence. Let $d_t = x_t - x_{t-1} = 
\delta + \epsilon_t$. 
Then $d_t \sim N(0 + \delta, \sigma^2)$ and $d_t$ is *i.i.d*. Since $\sigma$ is unknown,
we can use the *t-test* to test 
whether the mean of $d_t$ is zero, which means $\delta = 0$. The null distribution is $t_{n-1}$, 
where $n$ is the length of the random walk sequence. The test statistic is
\begin{align*}
    t & = \frac{\bar{d}}{\hat{\sigma}/\sqrt{n}}.
\end{align*}
We can compute it in R by using the function 
`t.test()`.

5. (2 pts)
Simulate a random walk of length 100 *without* drift, i.e., $\delta = 0$, and 
compute the test statistic you devised in Q4 and report its value. Then repeat,
but using a large nonzero value $\delta$.

```{r}
# CODE GOES HERE
# Generate some data
n <- 101
delta <- 0
sigma <- 1
epsilon <- rnorm(n, 0, sigma)
x <- rep(0, n)
for (t in 2:n) {
  x[t] <- delta + x[t - 1] + epsilon[t]
}

# First differences
diff_x <- diff(x)

# t-test
test_result <- t.test(diff_x, mu = 0, alternative = "two.sided")
t_value <- test_result$statistic
p_value <- test_result$p.value

print(test_result)
print(paste("test statistic:", t_value))
print(paste("likelihood of null hypothesis", p_value))


```

```{r}
# CODE GOES HERE
# Generate some data
# let drift be 10
n <- 101
delta <- 10
sigma <- 1
epsilon <- rnorm(n, 0, sigma)
x <- rep(0, n)
for (t in 2:n) {
  x[t] <- delta + x[t - 1] + epsilon[t]
}

# First differences
diff_x <- diff(x)

# t-test
test_result <- t.test(diff_x, mu = 0, alternative = "two.sided")
t_value <- test_result$statistic
p_value <- test_result$p.value

print(test_result)
print(paste("test statistic:", t_value))
print(paste("likelihood of null hypothesis", p_value))

```

6. (4 pts)
Simulate 100 random walks each of length 500, with nonzero drift, and plot them 
on the same plot using transparent coloring, following the code used in the 
lecture notes from week 2 ("Measures of dependence and stationarity"). Calculate
the sample mean $\hat\mu_t$ at each time $t$, across the repetitions, and plot 
as a dark line on the same plot. Then, calculate the sample standard deviation 
$\hat\sigma_t$ at each time $t$, and plot the mean plus or minus one standard 
deviation: $\hat\mu_t \pm \hat\sigma_t$, as dark dotted lines on the same plot.
Describe what you see (you should see that both the mean and variance
increase over time).

```{r}
# CODE GOES HERE
```

# Stationarity

7. (3 pts)
Compute the mean, variance, auto-covariance, and auto-correlation functions for
the process
\[
x_t = w_t w_{t-1},
\]
where each $w_t \sim N(0, \sigma^2)$, independently. Is $x_t$, $t = 1,2,3,\dots$ 
stationary?

SOLUTION GOES HERE

8. (3 pts)
Repeat the same calculations in Q7, but where each $w_t \sim N(\mu, \sigma^2)$,
independently, for $\mu \not= 0$. Is $x_t$, $t = 1,2,3,\dots$ stationary?

SOLUTION GOES HERE

9. (3 pts)
Simulate the processes from Q7 (with  $\mu = 0$) and Q8 (with $\mu \not= 0$) and 
plot the results. Compute the sample mean and sample variance for each simulated
process and check that it is close to the population mean and variance from Q7 
and Q8. Compute and plot the sample auto-correlation function using `acf()` and 
check again that it agrees with the population auto-correlation function from Q7
and Q8.

```{r}
# CODE GOES HERE
```

10. (2 pts)
Give an example of a weakly stationary process that is not strongly stationary.

SOLUTION GOES HERE

11. (Bonus)
A function $\kappa$ is said to be *positive semidefinite* (PSD) provided that
\[
\sum_{i,j=1}^n a_i a_j \kappa(t_i - t_j) \geq 0, \quad \text{for all $n \geq 1$, 
all $a_1,\dots,a_n$, and all $t_1,\dots,t_n$}.
\]
Prove that is $x_t$, $t = 1,2,3,\dots$ is stationary, and $\gamma_x(h)$ is its
auto-covariance function (as a function of lag $h$), then $\gamma_x$ is PSD.
You may use any equivalences between PSD and other linear-algebraic properties 
that you want, as long as you state clearly what you are using.

SOLUTION GOES HERE

# Joint stationarity
\def\eqd{\overset{d}{=}}

Notions of joint stationarity, between two time series, can be defined in an
analogous way to how we defined stationarity in lecture. We say that two time
series $x_t$, $t = 1,2,3,\dots$ and $y_t$, $t = 1,2,3,\dots$ are *strongly
jointly stationary* provided that:
\begin{multline*}
(x_{s_1}, x_{s_2}, \dots, x_{s_k}, y_{t_1}, y_{t_2}, \dots, y_{t_\ell}) 
\eqd (x_{s_1+h}, x_{s_2+h}, \dots, x_{s_k+h}, y_{t_1+h}, y_{t_2+h}, \dots, 
y_{t_\ell+h}), \\ \text{for all $k,\ell \geq 1$, all $s_1,\dots,s_k$ and
$t_1,\dots,t_\ell$, and all $h$}.
\end{multline*}
Here $\eqd$ means equality in distribution. In other words, any collection of
variates from the two sequences has the same joint distribution after we shift
the time indices forward or backwards in time. Meanwhile, we say that $x_t$, 
$t = 1,2,3,\dots$ and $y_t$, $t = 1,2,3,\dots$ are *weakly jointly stationary*
or simply *jointly stationary* provided that each series is stationary, and:
\[
\gamma_{xy}(s,t) = \gamma_{xy}(s+h, t+h), \quad \text{for all $s,t,h$}.
\]
Here $\gamma_{xy}$ is the cross-covariance function between $x,y$. In other 
words, the cross-covariance function must be invariant to shifts forward or 
backwards in time, and is only a function of the lag $h = s-t$. For jointly
stationary series, we can hence abbreviate their cross-covariance function by
$\gamma_{xy}(h)$. 

SOLUTION GOES HERE

12. (2 pts)
Give an example of two time series that are weakly jointly stationary but not 
strongly jointly stationary. 

SOLUTION GOES HERE

13. (3 pts)
If $x_t$, $t = 1,2,3,\dots$ and $y_t$, $t = 1,2,3,\dots$ form a *joint Gaussian
process*, which means that any collection $(x_{s_1}, x_{s_2}, \dots, x_{s_k}, 
y_{t_1}, y_{t_2}, \dots, y_{t_\ell})$ of variates along the series has a 
multivariate Gaussian distribution, then prove that weak joint stationarity 
implies strong joint stationarity.

SOLUTION GOES HERE

14. (3 pts)
Write down explicit formulas that shows how to estimate the cross-covariance 
and cross-correlation function of two finite time series $t = 1,\dots,n$ and 
$y_t$, $t = 1,\dots,n$, under the assumption of joint stationarity. Hint: these
should be analogous to the *sample auto-covariance and sample auto-correlation 
functions* that we covered in lecture.

SOLUTION GOES HERE

15. (4 pts) 
Following the code used in the lecture notes from week 2 ("Measures of 
dependence and stationarity"), use the `ccf()` function to compute and plot 
the sample cross-correlation function between Covid-19 cases and deaths, 
separately, for each of Florida, Georgia, New York, Pennsylvania, and Texas. 
(The lecture code does this for California.) Comment on what you find: do the
cross-correlation patterns look similar across different states? 

    Also, follow the lecture code to plot the case and death signals together, 
on the same plot, for each state (the lecture code provides a way to do this so
that they are scaled dynamically to attain the same min and max, and hence look 
nice when plotted together). Comment on whether the estimated cross-correlation
patterns agree with what you see visually between the case and death signals.

```{r}
# CODE GOES HERE
```
