\documentclass{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm} % Math notations imported
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage[margin=1in]{geometry}
\graphicspath{{./images/}} % Path to images

% \begin{figure}[htb!]
%      \centering
%      \includegraphics[scale=0.5]{coloring.png}
%      \caption{Coloring of the graph.}
% \end{figure}

% \begin{figure}[htb]
%     \qquad
%     \begin{minipage}{.4\textwidth}
%         \centering
%         \includegraphics[scale=0.35]{prims.png}
%         \caption{}
%     \end{minipage}    
%     \qquad
%     \begin{minipage}{.4\textwidth}
%         \centering
%         \includegraphics[scale=0.35]{kruskal.png}
%         \caption{}
%     \end{minipage}        
% \end{figure} 

\newtheorem{thm}{Theorem}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

% title information
\title{STAT 153 sketch}
\author{Neo Lee}
\date{}

\setstretch{1.15}
% main content
\begin{document} 

% placing title information; comment out if using fancyhdr
\maketitle 

Proof of null$(X)$ contains at least one non-zero vector $\eta$:

Since $p>n$, the column vectors are linear dependent. Denote $(v_1, \cdots, v_p)$ as the column 
vectors of $X$. Then, there are non trivial coefficients $(c_1, \cdots, c_p)$ such that
$$\sum_{i=1}^{p}c_iv_i=0.$$
Hence, $\eta = (c_1,\cdots, c_p)$ is a non-zero vector in null$(X)$.

Proof of $\hat{\beta}=\tilde{\beta}+\eta$ is also a least squares solution for $\eta\in 
\mathrm{null}(X)$:

Denote the prediction from $\tilde{\beta}$ as $\tilde{y} = X\tilde{\beta}$ with MSE = $y-\tilde{y}$. 
Then the prediction from $\hat{\beta}$
\begin{align}
    \hat{y} & = X\hat{\beta} \\
    & = X(\tilde{\beta}+\eta) \\
    & = X\tilde{\beta}+X\eta \\
    & = \tilde{y}+X\eta \\
    & = \tilde{y}.
\end{align}
Therefore, they have the same MSE. Since $\hat{\beta}$ is a least squares solution, $\tilde{\beta}+
\eta$ is also a least squares solution.



Since null$(X)\not\perp e_j$, there exists some $v\in$ null$(X)$ that has non-zero $j$-th 
coorindate. Denote the $j$-th coordinate of $v$ as a real number $c$. If $c>0$, we can construct 
$\hat{\beta}=\tilde{\beta} - \left(\frac{\tilde{\beta}_j}{c}\right)v - v$, which has the $j$-th coordinate 
less than 0. If $c<0$, we can construct 
$\hat{\beta}=\tilde{\beta}+\left(\frac{\tilde{\beta}_j}{c}\right)v-v$, which also has the $j$-th 
coordinate less than 0.

For either case, the prediction 
\begin{align*}
    \hat{y} & = X\tilde{\beta} \\
    & = X\left[\tilde{\beta}\pm \left(\frac{\tilde{\beta}_j}{c}\right)v-v\right] \\
    & = X\tilde{\beta} \pm X\left(\frac{\tilde{\beta}_j}{c}\right)v - Xv \\
    & = X\tilde{\beta} \pm \left(\frac{\tilde{\beta}_j}{c}\right)Xv - Xv \\
    & = X\tilde{\beta} \qquad (\because v \in \mathrm{null}(X)) \\
    & = \tilde{y}.
\end{align*}
Hence, $\tilde{\beta}$ and $\hat{\beta}$ will have the same prediction under $X$, so as the MSE.


\begin{align*}
    \mu_t & = \mathbb{E}(x_t) \\
    & = \mathbb{E}\left[\sum_{j=1}^p \Big( U_{j1} \cos(2\pi\omega_j t) + U_{j2} \sin(2\pi\omega_j t) \Big)\right] \\
    & = \mathbb{E}\left[\sum_{j=1}^p\left(U_{j1}+U{j2}\right)\right] \\
    & = 0.
\end{align*}


Notation: Use $S$ to represent $x_s$ and $T$ to represent $x_t$, so $S_{k1} = U_{k1}\cos(2\pi\omega_is)$, $S_{k2} = U_{k2}\sin(2\pi\omega_is)$, 
while 
$T_{k1} =
U_{k1}\cos(2\pi\omega_it)$, $T_{k2} = U_{k2}\sin(2\pi\omega_it)$. Notice the $S$ and $T$ are 
uncorrelated if they have different subscripts.
Then the auto-covariance is
\begin{align*}
    \gamma(s,t) & = \mathrm{Cov}\left(x_s,x_t\right) \\
    & = \mathrm{Cov}\left(\sum_{j=1}^p \Big( U_{j1} \cos(2\pi\omega_j s) + U_{j2} \sin(2\pi\omega_j s) \Big),
    \sum_{j=1}^p \Big( U_{j1} \cos(2\pi\omega_j t) + U_{j2} \sin(2\pi\omega_j t) \Big)\right) \\
    & = \sum_{j=1}^{p}\sum_{i=1}^{p}\mathrm{Cov}\left(S_{j1}, T_{i1}\right) + 
    \sum_{j=1}^{p}\sum_{i=1}^{p}\mathrm{Cov}\left(S_{j1}, T_{i2}\right) + 
    \sum_{j=1}^{p}\sum_{i=1}^{p}\mathrm{Cov}\left(S_{j2}, T_{i1}\right) + 
    \sum_{j=1}^{p}\sum_{i=1}^{p}\mathrm{Cov}\left(S_{j2}, T_{i2}\right) \\
    & = \sum_{i=1}^{p}\mathrm{Cov}\left(S_{i1}, T_{i1}\right) + 
    \sum_{i=1}^{p}\mathrm{Cov}\left(S_{i2}, T_{i2}\right) \\
    & = \sum_{i=1}^{p}\sigma^2\cos(2\pi\omega_is)\cos(2\pi\omega_it) +
    \sum_{i=1}^{p}\sigma^2\sin(2\pi\omega_is)\sin(2\pi\omega_it) \\
    & = \sigma^2\sum_{i=1}^{p}\left[\cos(2\pi\omega_is)\cos(2\pi\omega_it) +
    \sin(2\pi\omega_is)\sin(2\pi\omega_it)\right] \\
    & = \sigma^2\sum_{i=1}^{p}\cos(2\pi\omega_is - 2\pi\omega_it) \\
    & = \sigma^2\sum_{i=1}^{p}\cos(2\pi\omega_i(s-t)),
\end{align*}
which after reparametrizing to the lag $h$ is the same as 
$$\gamma(h) = \sigma^2\sum_{i=1}^{p}\cos(2\pi\omega_ih).$$

Since the auto-covariance is dependent on the lag only and mean is a constant 0, the process is 
weakly stationary.


\end{document}
