\documentclass{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm} % Math notations imported
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}

\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

% title information
\title{Probability Distribution Properties}

% main content
\begin{document} 
\date{}

% placing title information; comment out if using fancyhdr
\maketitle 
\section*{1. Discrete Distributions}
\subsection*{1.1 Uniform random variable}
\begin{align}
    & E[X] = \frac{x_1 + \dots + x_n}{n}; \quad Var(X) = \frac{x_1^2 + \dots + x_n^2}{n} - \left(\frac{x_1 + \dots + x_n}{n}\right)^2. \nonumber \\
    & E[X] = \frac{n+1}{2}; \quad Var(X) = \frac{n^2-1}{12}; \quad \emph{only when $x_i = [1, 2, \dots, n]$}. \nonumber
\end{align}

\subsection*{1.2 Bernoulli random variable}
\begin{align}
    & I = \begin{cases}
        1 & \text{if $X = 1$}, \\
        0 & \text{if $X = 0$}.
    \end{cases} \nonumber \\
    & E[I] = p; \quad Var(I) = p(1-p). \nonumber
\end{align}

\subsection*{1.3 Binomial random variable}
\begin{align}
    & P(X=k) = {n\choose k}p^k(1-p)^{n-k} \quad \emph{for } k \in [0,n]. \nonumber \\
    & E[X] = np; \quad Var(X) = np(1-p). \nonumber
\end{align}

\subsection*{1.4 Poisson random variable}
\begin{align}
    & P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!} \quad \emph{for } k \in \mathbb{Z}^{^\ge}. \nonumber \\
    & E[X] = \lambda; \quad Var(X) = \lambda. \nonumber
\end{align}

\subsection*{1.5 Geometric random variable}
\begin{align}
    & P(X=n) = (1-p)^{n-1}p \quad \emph{for } n \in \mathbb{N}. \nonumber \\
    & E[X] = \frac{1}{p}; \quad Var(X) = \frac{1-p}{p^2}; \quad P(X > n) = (1-p)^n ; \quad P(X>n+k|X>k) = P(X>n). \nonumber
\end{align}

\newpage
\section*{2. Continuous Distributions}
\subsection*{2.1 Uniform random variable}
\begin{align}
    & f(x) = \begin{cases}
        \frac{1}{\beta-\alpha} & \text{if $x \in [\alpha, \beta]$}, \\
        0 & \text{otherwise}.
    \end{cases} \nonumber \\
    & E[X] = \frac{\alpha+\beta}{2}; \quad Var(X) = \frac{(\beta-\alpha)^2}{12}. \nonumber
\end{align}

\subsection*{2.2 Exponential random variable}
\begin{align}
    & f(x) = \begin{cases}
        \lambda e^{-\lambda x} & \text{if $x \ge 0$}, \\
        0 & \text{otherwise}.
    \end{cases} \nonumber \\
    & E[X] = \frac{1}{\lambda}; \quad Var(X) = \frac{1}{\lambda^2}; \quad P(X\ge x) = e^{-\lambda x}; \quad P(X\ge x + y|X\ge y) = e^{-\lambda x}. \nonumber
\end{align}

\subsection*{2.3 Normal random variable}
\begin{align}
    & f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \quad \emph{for $x\in \mathbb{R}$}. \nonumber \\
    & E[X] = \mu; \quad Var(X) = \sigma^2; \quad Z \sim \emph{N(0,1)} = \frac{X-\mu}{\sigma}. \nonumber
\end{align}

\subsection*{2.4 Gamma random variable}
\begin{align}
    & f(x) = \frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x} \quad \emph{for $\alpha, \beta, x\in \mathbb{R}^{^+}$}; \quad \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt. \nonumber \\
    & E[X] = \frac{\alpha}{\lambda}; \quad Var(X) = \frac{\alpha}{\lambda^2}. \nonumber
\end{align}

\newpage
\section*{3. Multivariate Distributions}
\subsection*{3.1 Multivariate Normal Distribution}
\begin{align}
    & f(\vec{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\vec{x}-\vec{\mu})^{^\top}\Sigma^{-1}(\vec{x}-\vec{\mu})} \quad \emph{for $x\in \mathbb{R}^n$}. \nonumber \\
    & E[X] = \mu; \quad Var(X) = \Sigma. \nonumber
\end{align}

Let random vector $\begin{bmatrix} X_1 \\ \vdots \\ X_n \end{bmatrix}$ have a multivariate normal distribution and $Z$ be the linear combination of the random vector with coefficients $\vec{A} = \begin{bmatrix}
    a_1 \\ 
    \vdots \\
    a_n
\end{bmatrix}$.
Then, $Z\sim \emph{N($\vec{A}\cdot \vec{\mu},\vec{A}^{^\top}\sum\vec{A}$)}$.
For bivariate random vector $\begin{bmatrix}
    X \\ Y
\end{bmatrix}$ with $\vec{A} = \begin{bmatrix}
    a \\ b
\end{bmatrix}$,
\begin{align}
    Var(Z) = a^2\sigma_X^2+2ab\rho\sigma_X\sigma_Y+b^2\sigma_Y^2. \nonumber
\end{align}

\subsection*{3.2 Multinomial Distribution}
\begin{align}
    & P(\vec{x}) = \frac{n!}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k} \quad \emph{for $\vec{x} = \begin{bmatrix}
        x_1 \\ \vdots \\ x_k
    \end{bmatrix}$, $x_i\in \mathbb{N}$, $\sum_{i=1}^k x_i = n$, $p_i\in [0,1]$}. \nonumber \\
    & E[\vec{x}] = n\vec{p}; \quad Var(\vec{x}) = n\overrightarrow{p(1-p)}. \nonumber
\end{align}

\newpage
\section*{4. Theorems}
\subsection*{4.1 Chebyshev's Inequality}
\begin{align}
    & P(|X-\mu| \ge k\sigma) \le \frac{1}{k^2}. \nonumber
\end{align}

\subsection*{4.2 Markov's Inequality}
\begin{align}
    & P(X \ge k) \le \frac{E[X]}{k}. \nonumber
\end{align}

\subsection*{4.3 Central Limit Theorem}
Let $S_n = X_1 + \dots + X_n$. When $n$ is large, $S_n$ is approximately normally distributed with mean $n\mu $ and variance $n\sigma^2$.
\begin{align}
    & \frac{S_n-n\mu}{\sqrt{n}\sigma} = \frac{\bar{S_n} - \mu}{\sigma / \sqrt{n}} \sim \emph{N(0,1)} \quad \emph{as $n\to\infty$}. \nonumber
\end{align}

\subsection*{4.4 Weak Law of Large Numbers}
Let $S_n = X_1 + \dots + X_n$. 
\begin{align}
    & \lim_{n\rightarrow\infty}P(|\bar{S_n}-\mu|<\epsilon) = 1. \nonumber
\end{align}

\subsection*{4.5 Strong Law of Large Numbers}
Let $S_n = X_1 + \dots + X_n$.
\begin{align}
    & P(\lim_{n\rightarrow\infty}\bar{S_n}=\mu) = 1. \nonumber
\end{align}

\subsection*{4.6 Moment Generating Function}
\begin{align}
    & M_X(t) = E[e^{tX}] = \int_{-\infty}^\infty e^{tx}f(x)dx. \nonumber
\end{align}

\end{document}